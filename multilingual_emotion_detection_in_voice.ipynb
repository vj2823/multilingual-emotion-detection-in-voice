{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqOxboRrkKw5",
        "outputId": "ed3c4efb-0268-4d62-cc5d-a59e04aa43c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting openai-whisper\n",
            "  Using cached openai_whisper-20240930-py3-none-any.whl\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-cusolver-cu12-11.6.1.9 openai-whisper-20240930\n"
          ]
        }
      ],
      "source": [
        "pip install openai-whisper librosa scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from moviepy.editor import AudioFileClip\n",
        "\n",
        "# Step 1: Load Whisper Model for Speech Recognition\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Step 2: Transcribe Speech using Whisper\n",
        "def transcribe_audio(audio_path):\n",
        "    result = model.transcribe(audio_path)\n",
        "    print(\"Transcription: \", result[\"text\"])\n",
        "    return result[\"text\"], result[\"language\"]\n",
        "\n",
        "# Step 3: Extract Audio Features (MFCCs and Pitch) for Emotion Recognition\n",
        "def extract_audio_features(audio_path):\n",
        "    # Load audio using librosa\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Extract pitch\n",
        "    pitch, _ = librosa.core.piptrack(y=y, sr=sr)\n",
        "\n",
        "    # Ensure pitch has same length as MFCC (take mean of pitch over time)\n",
        "    pitch = np.mean(pitch, axis=1)\n",
        "\n",
        "    # Calculate mean and standard deviation for MFCCs and pitch as features\n",
        "    mfcc_features = np.mean(mfcc, axis=1)\n",
        "\n",
        "    # Limit pitch features to match number of MFCC features (13 features)\n",
        "    pitch_features = pitch[:13]  # Use first 13 pitch features\n",
        "\n",
        "    # Combine features\n",
        "    audio_features = np.concatenate([mfcc_features, pitch_features])\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "# Step 4: Train a Simple Emotion Recognition Model (You can train this model beforehand with labeled data)\n",
        "def train_emotion_classifier():\n",
        "    # Sample labeled data (features and emotions)\n",
        "    X = np.random.rand(100, 26)  # 13 MFCC + 13 pitch features\n",
        "    y = np.random.choice(['happy', 'sad', 'angry', 'neutral'], size=100)\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    # Train a simple classifier (Support Vector Machine)\n",
        "    classifier = SVC(kernel='linear')\n",
        "    classifier.fit(X, y_encoded)\n",
        "\n",
        "    return classifier, le\n",
        "\n",
        "# Step 5: Classify Emotion Based on Extracted Features\n",
        "def classify_emotion(features, classifier, le):\n",
        "    emotion_idx = classifier.predict([features])[0]\n",
        "    emotion = le.inverse_transform([emotion_idx])[0]\n",
        "    return emotion\n",
        "\n",
        "# Step 6: Convert MP4 to WAV using MoviePy\n",
        "def convert_mp4_to_wav(mp4_path, wav_path):\n",
        "    audio_clip = AudioFileClip(mp4_path)\n",
        "    audio_clip.write_audiofile(wav_path, codec='pcm_s16le')\n",
        "\n",
        "# Step 7: Combine Everything for Emotion-Aware Speech Recognition\n",
        "def emotion_aware_speech_recognition(mp4_path):\n",
        "    # Step 7.1: Convert MP4 to WAV\n",
        "    wav_path = \"/content/temp_audio.wav\"\n",
        "    convert_mp4_to_wav(mp4_path, wav_path)\n",
        "\n",
        "    # Step 7.2: Transcribe Audio\n",
        "    transcription, language = transcribe_audio(wav_path)\n",
        "\n",
        "    # Step 7.3: Extract Audio Features for Emotion Detection\n",
        "    audio_features = extract_audio_features(wav_path)\n",
        "\n",
        "    # Step 7.4: Classify Emotion Based on Audio Features\n",
        "    emotion = classify_emotion(audio_features, emotion_classifier, label_encoder)\n",
        "\n",
        "    # Step 7.5: Output Result\n",
        "    print(f\"Detected Emotion: {emotion}\")\n",
        "    print(f\"Transcription: {transcription}\")\n",
        "    print(f\"Language Detected: {language}\")\n",
        "\n",
        "# Step 8: Train the Emotion Classifier (Only once)\n",
        "emotion_classifier, label_encoder = train_emotion_classifier()\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/happy voice.mp4\"  # Replace with your actual file path\n",
        "emotion_aware_speech_recognition(audio_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRosZSCBkLto",
        "outputId": "a6527f07-3957-4c61-8d68-3f4cbc34adb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/temp_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Transcription:   Hello how are you? Good morning everyone have a nice day\n",
            "Detected Emotion: happy\n",
            "Transcription:  Hello how are you? Good morning everyone have a nice day\n",
            "Language Detected: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HeCX1b4malu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}